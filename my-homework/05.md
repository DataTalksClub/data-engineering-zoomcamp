# my-homework-5
#### I used a jupyter notebook--in a different un-tracked directory--to run this code, and copied the contents of the notebook over to this markdown file for consistency sake.

## Question 1: Install Spark and PySpark
- Install Spark
- Run PySpark
- Create a local spark session
- Execute `spark.__version__`

```python
import pyspark
from pyspark.sql import SparkSession

# Start a new Spark session:
spark = SparkSession.builder \
    .master("local[*]") \
    .appName("homework5") \
    .getOrCreate()

# Print pyspspark's version:
print('Currently using PySpark: v'+ pyspark.__version__)
```

```bash
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/03/04 21:58:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Currently using PySpark: v3.4.4
```


## Question 2: Yellow October 2024
What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches.
- Read the October 2024 Yellow into a Spark Dataframe.
- Repartition the Dataframe to 4 partitions and save it to parquet.

```bash
# get the parquet file and download it to the working directory
!wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet
```

```bash
--2025-03-04 18:55:41--  https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet
Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 52.84.160.213, 52.84.160.116, 52.84.160.84, ...
Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|52.84.160.213|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 64346071 (61M) [binary/octet-stream]
Saving to: ‘yellow_tripdata_2024-10.parquet’

yellow_tripdata_202 100%[===================>]  61.36M  64.8MB/s    in 0.9s    

2025-03-04 18:55:42 (64.8 MB/s) - ‘yellow_tripdata_2024-10.parquet’ saved [64346071/64346071]
```

```python
# read the parquet file into a spark dataframe
df = spark.read.parquet("yellow_tripdata_2024-10.parquet")

# partition the original parquet into 4 files and save it to a directory.
df.repartition(4).write.parquet('data/pq/yellow/2024/10/')
```

```bash
!ls -l /home/brukeg/spark/data/pq/yellow/2024/10
```

```bash
total 94388
-rw-r--r-- 1 brukeg brukeg        0 Mar  4 19:47 _SUCCESS
-rw-r--r-- 1 brukeg brukeg 24154000 Mar  4 19:47 part-00000-fd25962d-e236-463f-b1c2-1c6b59183b4b-c000.snappy.parquet
-rw-r--r-- 1 brukeg brukeg 24143003 Mar  4 19:47 part-00001-fd25962d-e236-463f-b1c2-1c6b59183b4b-c000.snappy.parquet
-rw-r--r-- 1 brukeg brukeg 24180654 Mar  4 19:47 part-00002-fd25962d-e236-463f-b1c2-1c6b59183b4b-c000.snappy.parquet
-rw-r--r-- 1 brukeg brukeg 24168482 Mar  4 19:47 part-00003-fd25962d-e236-463f-b1c2-1c6b59183b4b-c000.snappy.parquet
```

```python
import os
import numpy as np

dir = "/home/brukeg/spark/data/pq/yellow/2024/10"

# Isolate the 4 parquet files and collect their sizes in a list
file_sizes = [
    os.path.getsize(os.path.join(dir, f)) 
    for f in os.listdir(dir) if f.endswith(".parquet")
]

# Calculate the average size of the 4 parquet files
average_size = np.mean(file_sizes) if file_sizes else 0

print(f'Average file size: {average_size:.2f} bytes')
```

```bash
import os
import numpy as np

dir = "/home/brukeg/spark/data/pq/yellow/2024/10"

# Isolate the 4 parquet files and collect their sizes in a list
file_sizes = [
    os.path.getsize(os.path.join(dir, f)) 
    for f in os.listdir(dir) if f.endswith(".parquet")
]

# Calculate the average size of the 4 parquet files
average_size = np.mean(file_sizes) if file_sizes else 0

print(f"Average file size: {average_size:.2f} bytes")
```


## Question 3: Count records
How many taxi trips were there on the 15th of October?

```python
from pyspark.sql import functions as F
from pyspark.sql import types


schema = types.StructType([
    types.StructField("VendorID", types.IntegerType(), True),
    types.StructField("tpep_pickup_datetime", types.TimestampType(), True),
    types.StructField("tpep_dropoff_datetime", types.TimestampType(), True),
    types.StructField("passenger_count", types.IntegerType(), True),
    types.StructField("trip_distance", types.DoubleType(), True),
    types.StructField("RatecodeID", types.IntegerType(), True),
    types.StructField("store_and_fwd_flag", types.StringType(), True),
    types.StructField("PULocationID", types.IntegerType(), True),
    types.StructField("DOLocationID", types.IntegerType(), True),
    types.StructField("payment_type", types.IntegerType(), True),
    types.StructField("fare_amount", types.DoubleType(), True),
    types.StructField("extra", types.DoubleType(), True),
    types.StructField("mta_tax", types.DoubleType(), True),
    types.StructField("tip_amount", types.DoubleType(), True),
    types.StructField("tolls_amount", types.DoubleType(), True),
    types.StructField("improvement_surcharge", types.DoubleType(), True),
    types.StructField("total_amount", types.DoubleType(), True),
    types.StructField("congestion_surcharge", types.DoubleType(), True)
])

df_yellow = spark.read \
    .schema(schema) \
    .parquet('data/pq/yellow/2024/*', mode='overwrite')

df_yellow.filter(
    (F.to_date("tpep_pickup_datetime") == "2024-10-15") &
    (F.col("trip_distance") > 0)
).count()
```

```bash
126106
```


## Question 4: Longest trip
What is the length of the longest trip in the dataset in hours?

```python
from pyspark.sql.functions import unix_timestamp

df_with_duration = df.withColumn("trip_duration_hours", 
    (unix_timestamp("tpep_dropoff_datetime") - unix_timestamp("tpep_pickup_datetime")) / 3600
    )

longest_trip = df_with_duration.agg(F.max("trip_duration_hours")).collect()[0][0]

print(f'Longest trip duration: {longest_trip} hours')
```

```bash
[Stage 15:=============================>                            (1 + 1) / 2]
Longest trip duration: 162.61777777777777 hours
```


## Question 5: User Interface
Spark’s User Interface which shows the application's dashboard runs on which local port?

**Answer: http://localhost:4040/**


## Question 6: Least frequent pickup location zone
Using the zone lookup data and the Yellow October 2024 data, what is the name of the LEAST frequent pickup location Zone?
- Load the zone lookup data into a temp view in Spark

```bash
# get the csv file and download it to the working directory
!wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv
```

```python
# read the csv file into a spark dataframe
df_zones = spark.read \
    .option("header", "true") \
    .csv('taxi_zone_lookup.csv')

# Create a temporary view
df_zones.createOrReplaceTempView("taxi_zones")

# Join zones and yellow into a single dataframe
df_result = df_yellow.join(df_zones, df_yellow.PULocationID == df_zones.LocationID)

# Group by Zone and count records (+alias the count column)
zone_count = df_result \
    .groupBy("Zone") \
    .agg(F.count("VendorID") \
    .alias("count"))

# Order by count ascending and limit to 1 record for the least frequent zone
least_frequent_zone = zone_count \
    .orderBy("count") \
    .limit(1) \
    .collect()[0]["Zone"]


print(f'The least frequent pickup location Zone is: {least_frequent_zone_name}')
```

```bash
[Stage 41:>                                                         (0 + 2) / 2]
The least frequent pickup location Zone is: Governor's Island/Ellis Island/Liberty Island
```
