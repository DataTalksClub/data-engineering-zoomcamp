{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RGdjQnN7J8A"
      },
      "source": [
        "# **Install `dlt`â³**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pai77xXpOyhU"
      },
      "source": [
        "What is dlt?\n",
        "\n",
        "* dlt is an open-source library that you can add to your Python scripts to load data from various and often messy data sources into well-structured, live datasets.\n",
        "* You can install it using pip and there's no need to start any backends or containers. You can simply import dlt in your Python script and write a simple pipeline to load data from sources like APIs, databases, files, etc. into a destination of your choice.\n",
        "\n",
        "Here are a few reasons why you should use dlt:\n",
        "\n",
        "* Automated maintenance: With schema inference and evolution and alerts, and with short declarative code, maintenance becomes simple.\n",
        "* Run it where Python runs: You can use dlt on Airflow, serverless functions, notebooks. It doesn't require external APIs, backends or containers, and scales on both micro and large infrastructures.\n",
        "* User-friendly, declarative interface: dlt provides a user-friendly interface that removes knowledge obstacles for beginners while empowering senior professionals.\n",
        "\n",
        "Benefits: As a data engineer, dlt offers several benefits:\n",
        "\n",
        "* Efficient Data Extraction and Loading: dlt simplifies the process of extracting and loading data. It allows you to decorate your data-producing functions with loading or incremental extraction metadata, enabling dlt to extract and load data according to your custom logic. This is particularly useful when dealing with large datasets, as dlt supports scalability through iterators, chunking, and parallelization. Read more\n",
        "\n",
        "* Automated Schema Management: dlt automatically infers a schema from data and loads the data to the destination. It can easily adapt and structure data as it evolves, reducing the time spent on maintenance and development. This ensures data consistency and quality. Read more\n",
        "* Data Governance Support: dlt pipelines offer robust governance support through pipeline metadata utilization, schema enforcement and curation, and schema change alerts. This promotes data consistency, traceability, and control throughout the data processing lifecycle. Read more\n",
        "\n",
        "* Flexibility and Scalability: dlt can be used on Airflow, serverless functions, notebooks, and scales on both micro and large infrastructures. It also offers several mechanisms and configuration options to scale up and fine-tune pipelines. Read more\n",
        "\n",
        "* Post-Loading Transformations: dlt provides several options for transformations after loading the data, including using dbt, the dlt SQL client, or Pandas. This allows you to shape and manipulate the data before or after loading it, allowing you to meet specific requirements and ensure data quality and consistency. Read more\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UvUQO__kgUP"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install dlt[duckdb] # Install dlt with all the necessary DuckDB dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYweO0LGOQjZ"
      },
      "source": [
        "# Part 1: Data Extraction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Awoh5o-Yo_Ve"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqFgA2AipT-f"
      },
      "source": [
        "## Example 1: Extracting API data with a generator\n",
        "\n",
        "Premise:\n",
        "\n",
        "For this example, we created a simple http api that returns json \"page by page\",  1000 records per page.\n",
        "\n",
        "It accepts a parameter called `page`, representing the page number.\n",
        "If we request a larger page number than there is data, we get an empty response.\n",
        "\n",
        "To get the pages, we write a loop that asks for pages starting from 1 and increasing, until we receive an empty page.\n",
        "\n",
        "As we do not know ahead of time how many pages have data and if they fit in memory, yielding the data so it can be handled page by page scales better than first collecting all pages in memory and then returning them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zBFy18_Sa6d",
        "outputId": "96ab9c2d-b5cb-47f0-bded-22632cc98e20"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "\n",
        "\n",
        "BASE_API_URL = \"https://us-central1-dlthub-analytics.cloudfunctions.net/data_engineering_zoomcamp_api\"\n",
        "\n",
        "# I call this a paginated getter\n",
        "# as it's a function that gets data\n",
        "# and also paginates until there is no more data\n",
        "# by yielding pages, we \"microbatch\", which speeds up downstream processing\n",
        "\n",
        "def paginated_getter():\n",
        "    page_number = 1\n",
        "\n",
        "    while True:\n",
        "        # Set the query parameters\n",
        "        params = {'page': page_number}\n",
        "\n",
        "        # Make the GET request to the API\n",
        "        response = requests.get(BASE_API_URL, params=params)\n",
        "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
        "        page_json = response.json()\n",
        "        print(f'got page number {page_number} with {len(page_json)} records')\n",
        "\n",
        "        # if the page has no records, stop iterating\n",
        "        if page_json:\n",
        "            yield page_json\n",
        "            page_number += 1\n",
        "        else:\n",
        "            # No more data, break the loop\n",
        "            break\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Use the generator to iterate over pages\n",
        "    for page_data in paginated_getter():\n",
        "        # Process each page as needed\n",
        "        print(page_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgI8VPNCrdGi"
      },
      "source": [
        "## Example 2: The \"bad\" way to download a file\n",
        "\n",
        "In this example we download a json lines file.\n",
        "\n",
        "Since the download is text but we want to work with iterable data strutures for loading, we convert the contents to list of jsons.\n",
        "\n",
        "This is a less than ideal approach because if the file size is unknown, we run the risk of running out of memory. In the case of machines that run multiple jobs, an out of memory error runs the risk of killing not just the current jobs but also anything else running on the machine at the time - a situation most data engineers **really really** like to avoid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW4AI4yerkTj",
        "outputId": "c4d87ad0-e5cf-442f-f495-9112cdc3b926"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "\n",
        "def download_and_read_jsonl(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raise an HTTPError for bad responses\n",
        "    data = response.text.splitlines()\n",
        "    parsed_data = [json.loads(line) for line in data]\n",
        "    return parsed_data\n",
        "\n",
        "\n",
        "# time the download\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "url = \"https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl\"\n",
        "downloaded_data = download_and_read_jsonl(url)\n",
        "\n",
        "if downloaded_data:\n",
        "    # Process or print the downloaded data as needed\n",
        "    print(downloaded_data[:5])  # Print the first 5 entries as an example\n",
        "\n",
        "# time the download\n",
        "end = time.time()\n",
        "print(end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rloz9PKQq5-g"
      },
      "source": [
        "## Example 3: Extracting file data with a generator \"the best practice way\"\n",
        "\n",
        "\"The best practice way\" here refers to the most scalable way to do it, but if you are confident scale will not be an issue, then the right way might be the simplest :)\n",
        "\n",
        "In this example we download a jsonl (like json, but lines) file.\n",
        "Since it's jsonl, it has lines so we can process it line by line.\n",
        "\n",
        "We stream download it and yield the data.\n",
        "\n",
        "If this file were json and not jsonl, we could use ijson library to break it into lines without loading to memory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29-plLIHOPsP",
        "outputId": "a61fac96-3df7-4435-a733-b3100bc73a0d"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "url = \"https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl\"\n",
        "\n",
        "def stream_download_jsonl(url):\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status()  # Raise an HTTPError for bad responses\n",
        "    for line in response.iter_lines():\n",
        "        if line:\n",
        "            yield json.loads(line)\n",
        "\n",
        "# time the download\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "# Use the generator to iterate over rows with minimal memory usage\n",
        "row_counter = 0\n",
        "for row in stream_download_jsonl(url):\n",
        "    print(row)\n",
        "    row_counter += 1\n",
        "    if row_counter >= 5:\n",
        "        break\n",
        "\n",
        "# time the download\n",
        "end = time.time()\n",
        "print(end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM7qqu1dOrop"
      },
      "source": [
        "### Loading the generator (any of the above)\n",
        "\n",
        "We have 3 ways to download the same data. Let's use the fast and reliable way to load some data and inspect it in DuckDB.\n",
        "\n",
        "In this example, we are using `dlt` library to do the loading, which will process data from the generators incrementally, following the same memory management paradigm.\n",
        "\n",
        "We will discuss more details about `dlt` or \"data load tool\" later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9PrR_edOvSw",
        "outputId": "3e44149a-eca2-411f-b013-6ec60be324c5"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "\n",
        "# define the connection to load to.\n",
        "# We now use duckdb, but you can switch to Bigquery later\n",
        "generators_pipeline = dlt.pipeline(destination='duckdb', dataset_name='generators')\n",
        "\n",
        "\n",
        "# we can load any generator to a table at the pipeline destnation as follows:\n",
        "info = generators_pipeline.run(paginated_getter(),\n",
        "\t\t\t\t\t\t\t\t\t\ttable_name=\"http_download\",\n",
        "\t\t\t\t\t\t\t\t\t\twrite_disposition=\"replace\")\n",
        "\n",
        "# the outcome metadata is returned by the load and we can inspect it by printing it.\n",
        "print(info)\n",
        "\n",
        "# we can load the next generator to the same or to a different table.\n",
        "info = generators_pipeline.run(stream_download_jsonl(url),\n",
        "\t\t\t\t\t\t\t\t\t\ttable_name=\"stream_download\",\n",
        "\t\t\t\t\t\t\t\t\t\twrite_disposition=\"replace\")\n",
        "\n",
        "print(info)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QSDNShTI45qC",
        "outputId": "a916c263-1897-4107-fb73-780b604a7d49"
      },
      "outputs": [],
      "source": [
        "# show outcome\n",
        "\n",
        "import duckdb\n",
        "\n",
        "conn = duckdb.connect(f\"{generators_pipeline.pipeline_name}.duckdb\")\n",
        "\n",
        "# let's see the tables\n",
        "conn.sql(f\"SET search_path = '{generators_pipeline.dataset_name}'\")\n",
        "print('Loaded tables: ')\n",
        "display(conn.sql(\"show tables\"))\n",
        "\n",
        "# and the data\n",
        "\n",
        "print(\"\\n\\n\\n http_download table below:\")\n",
        "\n",
        "rides = conn.sql(\"SELECT * FROM http_download\").df()\n",
        "display(rides)\n",
        "\n",
        "print(\"\\n\\n\\n stream_download table below:\")\n",
        "\n",
        "passengers = conn.sql(\"SELECT * FROM stream_download\").df()\n",
        "display(passengers)\n",
        "\n",
        "# As you can see, the same data was loaded in both cases."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
