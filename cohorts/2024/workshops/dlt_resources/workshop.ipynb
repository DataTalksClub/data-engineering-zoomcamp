{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RGdjQnN7J8A"
      },
      "source": [
        "# **Install `dlt`⏳**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pai77xXpOyhU"
      },
      "source": [
        "What is dlt?\n",
        "\n",
        "* dlt is an open-source library that you can add to your Python scripts to load data from various and often messy data sources into well-structured, live datasets.\n",
        "* You can install it using pip and there's no need to start any backends or containers. You can simply import dlt in your Python script and write a simple pipeline to load data from sources like APIs, databases, files, etc. into a destination of your choice.\n",
        "\n",
        "Here are a few reasons why you should use dlt:\n",
        "\n",
        "* Automated maintenance: With schema inference and evolution and alerts, and with short declarative code, maintenance becomes simple.\n",
        "* Run it where Python runs: You can use dlt on Airflow, serverless functions, notebooks. It doesn't require external APIs, backends or containers, and scales on both micro and large infrastructures.\n",
        "* User-friendly, declarative interface: dlt provides a user-friendly interface that removes knowledge obstacles for beginners while empowering senior professionals.\n",
        "\n",
        "Benefits: As a data engineer, dlt offers several benefits:\n",
        "\n",
        "* Efficient Data Extraction and Loading: dlt simplifies the process of extracting and loading data. It allows you to decorate your data-producing functions with loading or incremental extraction metadata, enabling dlt to extract and load data according to your custom logic. This is particularly useful when dealing with large datasets, as dlt supports scalability through iterators, chunking, and parallelization. Read more\n",
        "\n",
        "* Automated Schema Management: dlt automatically infers a schema from data and loads the data to the destination. It can easily adapt and structure data as it evolves, reducing the time spent on maintenance and development. This ensures data consistency and quality. Read more\n",
        "* Data Governance Support: dlt pipelines offer robust governance support through pipeline metadata utilization, schema enforcement and curation, and schema change alerts. This promotes data consistency, traceability, and control throughout the data processing lifecycle. Read more\n",
        "\n",
        "* Flexibility and Scalability: dlt can be used on Airflow, serverless functions, notebooks, and scales on both micro and large infrastructures. It also offers several mechanisms and configuration options to scale up and fine-tune pipelines. Read more\n",
        "\n",
        "* Post-Loading Transformations: dlt provides several options for transformations after loading the data, including using dbt, the dlt SQL client, or Pandas. This allows you to shape and manipulate the data before or after loading it, allowing you to meet specific requirements and ensure data quality and consistency. Read more\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UvUQO__kgUP"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install dlt[duckdb] # Install dlt with all the necessary DuckDB dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYweO0LGOQjZ"
      },
      "source": [
        "# Part 1: Data Extraction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Awoh5o-Yo_Ve"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqFgA2AipT-f"
      },
      "source": [
        "## Example 1: Extracting API data with a generator\n",
        "\n",
        "Premise:\n",
        "\n",
        "For this example, we created a simple http api that returns json \"page by page\",  1000 records per page.\n",
        "\n",
        "It accepts a parameter called `page`, representing the page number.\n",
        "If we request a larger page number than there is data, we get an empty response.\n",
        "\n",
        "To get the pages, we write a loop that asks for pages starting from 1 and increasing, until we receive an empty page.\n",
        "\n",
        "As we do not know ahead of time how many pages have data and if they fit in memory, yielding the data so it can be handled page by page scales better than first collecting all pages in memory and then returning them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zBFy18_Sa6d",
        "outputId": "46a21b1c-eb8a-46f9-f323-c59a5e0b4228"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "\n",
        "\n",
        "BASE_API_URL = \"https://us-central1-dlthub-analytics.cloudfunctions.net/data_engineering_zoomcamp_api\"\n",
        "\n",
        "# I call this a paginated getter\n",
        "# as it's a function that gets data\n",
        "# and also paginates until there is no more data\n",
        "# by yielding pages, we \"microbatch\", which speeds up downstream processing\n",
        "\n",
        "def paginated_getter():\n",
        "    page_number = 1\n",
        "\n",
        "    while True:\n",
        "        # Set the query parameters\n",
        "        params = {'page': page_number}\n",
        "\n",
        "        # Make the GET request to the API\n",
        "        response = requests.get(BASE_API_URL, params=params)\n",
        "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
        "        page_json = response.json()\n",
        "        print(f'got page number {page_number} with {len(page_json)} records')\n",
        "\n",
        "        # if the page has no records, stop iterating\n",
        "        if page_json:\n",
        "            yield page_json\n",
        "            page_number += 1\n",
        "        else:\n",
        "            # No more data, break the loop\n",
        "            break\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Use the generator to iterate over pages\n",
        "    for page_data in paginated_getter():\n",
        "        # Process each page as needed\n",
        "        print(page_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgI8VPNCrdGi"
      },
      "source": [
        "## Example 2: The \"bad\" way to download a file\n",
        "\n",
        "In this example we download a json lines file.\n",
        "\n",
        "Since the download is text but we want to work with iterable data strutures for loading, we convert the contents to list of jsons.\n",
        "\n",
        "This is a less than ideal approach because if the file size is unknown, we run the risk of running out of memory. In the case of machines that run multiple jobs, an out of memory error runs the risk of killing not just the current jobs but also anything else running on the machine at the time - a situation most data engineers **really really** like to avoid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW4AI4yerkTj",
        "outputId": "5aece05f-e1d4-49ff-e87e-196bb31947fc"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "\n",
        "def download_and_read_jsonl(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raise an HTTPError for bad responses\n",
        "    data = response.text.splitlines()\n",
        "    parsed_data = [json.loads(line) for line in data]\n",
        "    return parsed_data\n",
        "\n",
        "\n",
        "# time the download\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "url = \"https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl\"\n",
        "downloaded_data = download_and_read_jsonl(url)\n",
        "\n",
        "if downloaded_data:\n",
        "    # Process or print the downloaded data as needed\n",
        "    print(downloaded_data[:5])  # Print the first 5 entries as an example\n",
        "\n",
        "# time the download\n",
        "end = time.time()\n",
        "print(end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rloz9PKQq5-g"
      },
      "source": [
        "## Example 3: Extracting file data with a generator \"the best practice way\"\n",
        "\n",
        "\"The best practice way\" here refers to the most scalable way to do it, but if you are confident scale will not be an issue, then the right way might be the simplest :)\n",
        "\n",
        "In this example we download a jsonl (like json, but lines) file.\n",
        "Since it's jsonl, it has lines so we can process it line by line.\n",
        "\n",
        "We stream download it and yield the data.\n",
        "\n",
        "If this file were json and not jsonl, we could use ijson library to break it into lines without loading to memory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29-plLIHOPsP"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "url = \"https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl\"\n",
        "\n",
        "def stream_download_jsonl(url):\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status()  # Raise an HTTPError for bad responses\n",
        "    for line in response.iter_lines():\n",
        "        if line:\n",
        "            yield json.loads(line)\n",
        "\n",
        "# time the download\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "# Use the generator to iterate over rows with minimal memory usage\n",
        "row_counter = 0\n",
        "for row in stream_download_jsonl(url):\n",
        "    print(row)\n",
        "    row_counter += 1\n",
        "    if row_counter >= 5:\n",
        "        break\n",
        "\n",
        "# time the download\n",
        "end = time.time()\n",
        "print(end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM7qqu1dOrop"
      },
      "source": [
        "### Loading the generator (any of the above)\n",
        "\n",
        "We have 3 ways to download the same data. Let's use the fast and reliable way to load some data and inspect it in DuckDB.\n",
        "\n",
        "In this example, we are using `dlt` library to do the loading, which will process data from the generators incrementally, following the same memory management paradigm.\n",
        "\n",
        "We will discuss more details about `dlt` or \"data load tool\" later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9PrR_edOvSw",
        "outputId": "03754c32-2072-4742-9523-44826e7430c4"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "\n",
        "# define the connection to load to.\n",
        "# We now use duckdb, but you can switch to Bigquery later\n",
        "generators_pipeline = dlt.pipeline(destination='duckdb', dataset_name='generators')\n",
        "\n",
        "\n",
        "# we can load any generator to a table at the pipeline destnation as follows:\n",
        "info = generators_pipeline.run(paginated_getter(),\n",
        "\t\t\t\t\t\t\t\t\t\ttable_name=\"http_download\",\n",
        "\t\t\t\t\t\t\t\t\t\twrite_disposition=\"replace\")\n",
        "\n",
        "# the outcome metadata is returned by the load and we can inspect it by printing it.\n",
        "print(info)\n",
        "\n",
        "# we can load the next generator to the same or to a different table.\n",
        "info = generators_pipeline.run(stream_download_jsonl(url),\n",
        "\t\t\t\t\t\t\t\t\t\ttable_name=\"stream_download\",\n",
        "\t\t\t\t\t\t\t\t\t\twrite_disposition=\"replace\")\n",
        "\n",
        "print(info)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QSDNShTI45qC",
        "outputId": "e949cc3c-a1cb-4cf9-d2a2-2869797ab000"
      },
      "outputs": [],
      "source": [
        "# show outcome\n",
        "\n",
        "import duckdb\n",
        "\n",
        "conn = duckdb.connect(f\"{generators_pipeline.pipeline_name}.duckdb\")\n",
        "\n",
        "# let's see the tables\n",
        "conn.sql(f\"SET search_path = '{generators_pipeline.dataset_name}'\")\n",
        "print('Loaded tables: ')\n",
        "display(conn.sql(\"show tables\"))\n",
        "\n",
        "# and the data\n",
        "\n",
        "print(\"\\n\\n\\n http_download table below:\")\n",
        "\n",
        "rides = conn.sql(\"SELECT * FROM http_download\").df()\n",
        "display(rides)\n",
        "\n",
        "print(\"\\n\\n\\n stream_download table below:\")\n",
        "\n",
        "passengers = conn.sql(\"SELECT * FROM stream_download\").df()\n",
        "display(passengers)\n",
        "\n",
        "# As you can see, the same data was loaded in both cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzi0xTYb4Duq"
      },
      "source": [
        "# Part 2: Normalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRwRYRLce9i6"
      },
      "source": [
        "## Load nested data with auto normalisation\n",
        "\n",
        "When converting nested data to tabular formats, to keep fragmentations minimal:\n",
        "* Nested dictionaries can be flattened into the parent row to\n",
        "* Nested lists however need to be expressed as separate tables due to the different granularity (1:n relationship)\n",
        "\n",
        "And of course, when going from JSON to DB, we want some things standardised:\n",
        "* Data types such as timestamps should be detected correctly\n",
        "* Column names should be converted to db-compatible names\n",
        "* Unnested sub-tables should be linked to parent tables via auto generated keys\n",
        "\n",
        "\n",
        "For this work, we will use `dlt` library, which is purpose-made to solve such tasks in a scalable way, for example by using generators.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoTtwE5het4C"
      },
      "source": [
        "### Introducing dlt\n",
        "\n",
        "dlt is a python library created for the purpose of assisting data engineers to build simpler, faster and more robust pipelines with minimal effort.\n",
        "\n",
        "dlt automates much of the tedious work a data engineer would do, and does it in a way that is robust.\n",
        "\n",
        "dlt can handle things like:\n",
        "\n",
        "- Schema: Inferring and evolving schema, alerting changes, using schemas as data contracts.\n",
        "- Typing data, flattening structures, renaming columns to fit database standards.\n",
        "- Processing a stream of events/rows without filling memory. This includes extraction from generators. In our example we will pass the “data” you can see above.\n",
        "- Loading to a variety of dbs of file formats.\n",
        "\n",
        "Read more about dlt [here](https://dlthub.com/docs/intro).\n",
        "\n",
        "Now let’s use it to load our nested json to duckdb:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2i07XAdb67H5",
        "outputId": "9e6384ac-47b5-450f-c217-e38c935a8ba1"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "import duckdb\n",
        "\n",
        "data = [\n",
        "    {\n",
        "        \"vendor_name\": \"VTS\",\n",
        "\t\t\t\t\"record_hash\": \"b00361a396177a9cb410ff61f20015ad\",\n",
        "        \"time\": {\n",
        "            \"pickup\": \"2009-06-14 23:23:00\",\n",
        "            \"dropoff\": \"2009-06-14 23:48:00\"\n",
        "        },\n",
        "        \"Trip_Distance\": 17.52,\n",
        "        # nested dictionaries could be flattened\n",
        "        \"coordinates\": {\n",
        "            \"start\": {\n",
        "                \"lon\": -73.787442,\n",
        "                \"lat\": 40.641525\n",
        "            },\n",
        "            \"end\": {\n",
        "                \"lon\": -73.980072,\n",
        "                \"lat\": 40.742963\n",
        "            }\n",
        "        },\n",
        "        \"Rate_Code\": None,\n",
        "        \"store_and_forward\": None,\n",
        "        \"Payment\": {\n",
        "            \"type\": \"Credit\",\n",
        "            \"amt\": 20.5,\n",
        "            \"surcharge\": 0,\n",
        "            \"mta_tax\": None,\n",
        "            \"tip\": 9,\n",
        "            \"tolls\": 4.15,\n",
        "\t\t\t\t\t\t\"status\": \"booked\"\n",
        "        },\n",
        "        \"Passenger_Count\": 2,\n",
        "        # nested lists need to be expressed as separate tables\n",
        "        \"passengers\": [\n",
        "            {\"name\": \"John\", \"rating\": 4.9},\n",
        "            {\"name\": \"Jack\", \"rating\": 3.9}\n",
        "        ],\n",
        "        \"Stops\": [\n",
        "            {\"lon\": -73.6, \"lat\": 40.6},\n",
        "            {\"lon\": -73.5, \"lat\": 40.5}\n",
        "        ]\n",
        "    },\n",
        "    # ... more data\n",
        "]\n",
        "\n",
        "\n",
        "# define the connection to load to.\n",
        "# We now use duckdb, but you can switch to Bigquery later\n",
        "pipeline = dlt.pipeline(destination='duckdb', dataset_name='taxi_rides')\n",
        "\n",
        "\n",
        "\n",
        "# run with merge write disposition.\n",
        "# This is so scaffolding is created for the next example,\n",
        "# where we look at merging data\n",
        "\n",
        "info = pipeline.run(data,\n",
        "\t\t\t\t\t\t\t\t\t\ttable_name=\"rides\",\n",
        "\t\t\t\t\t\t\t\t\t\twrite_disposition=\"merge\",\n",
        "                    primary_key=\"record_hash\")\n",
        "\n",
        "print(info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbORAD3Rhsr1"
      },
      "source": [
        "### Inspecting the nested structure, joining the child tables\n",
        "\n",
        "Let's look at what happened during the load\n",
        "- By looking at the loaded tables, we can see our json document got flattened and sub-documents got split into separate tables\n",
        "- We can re-join those child tables to the parent table by using the generated keys `on parent_table._dlt_id = child_table._dlt_parent_id`.\n",
        "- Data types: If you will pay attention to datatypes, you will note that the timestamps, which in json are of string type, are now of timestamp type in the db.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Irf4XsfcgqQv",
        "outputId": "335c42e7-0c10-45b4-bb4a-eb68df5df96a"
      },
      "outputs": [],
      "source": [
        "# show the outcome\n",
        "\n",
        "conn = duckdb.connect(f\"{pipeline.pipeline_name}.duckdb\")\n",
        "\n",
        "# let's see the tables\n",
        "conn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\n",
        "print('Loaded tables: ')\n",
        "display(conn.sql(\"show tables\"))\n",
        "\n",
        "\n",
        "print(\"\\n\\n\\n Rides table below: Note the times are properly typed\")\n",
        "rides = conn.sql(\"SELECT * FROM rides\").df()\n",
        "display(rides)\n",
        "\n",
        "print(\"\\n\\n\\n Pasengers table\")\n",
        "passengers = conn.sql(\"SELECT * FROM rides__passengers\").df()\n",
        "display(passengers)\n",
        "print(\"\\n\\n\\n Stops table\")\n",
        "stops = conn.sql(\"SELECT * FROM rides__stops\").df()\n",
        "display(stops)\n",
        "\n",
        "\n",
        "# to reflect the relationships between parent and child rows, let's join them\n",
        "# of course this will have 4 rows due to the two 1:n joins\n",
        "\n",
        "print(\"\\n\\n\\n joined table\")\n",
        "\n",
        "joined = conn.sql(\"\"\"\n",
        "SELECT *\n",
        "FROM rides as r\n",
        "left join rides__passengers as rp\n",
        "  on r._dlt_id = rp._dlt_parent_id\n",
        "left join rides__stops as rs\n",
        "  on r._dlt_id = rs._dlt_parent_id\n",
        "\"\"\").df()\n",
        "display(joined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duR1wFIfbM07"
      },
      "source": [
        "What are we looking at?\n",
        "- Nested dicts got flattened into the parent row, the structure `{\"coordinates\":{\"start\": {\"lat\": ...}}}` became\n",
        "`coordinates__start__lat`\n",
        "\n",
        "- Nested lists got broken out into separate tables with generated columns that would allow us to join the data back when needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDpgADmSBdr9"
      },
      "source": [
        "# Part 3: Incremental loading\n",
        "## Update nested data\n",
        "\n",
        "In this example the scores of the 2 passengers changed. Turns out their payment didn't go through for the ride before and they got a bad rating from the driver, so now we have to update their rating.\n",
        "\n",
        "As you can see after running the code, their ratings are now lowered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "id": "D3W5iXow8cs8",
        "outputId": "66be5c20-86f4-4de8-9b66-28b3e84435a3"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "import duckdb\n",
        "\n",
        "data = [\n",
        "    {\n",
        "        \"vendor_name\": \"VTS\",\n",
        "\t\t\t\t\"record_hash\": \"b00361a396177a9cb410ff61f20015ad\",\n",
        "        \"time\": {\n",
        "            \"pickup\": \"2009-06-14 23:23:00\",\n",
        "            \"dropoff\": \"2009-06-14 23:48:00\"\n",
        "        },\n",
        "        \"Trip_Distance\": 17.52,\n",
        "        \"coordinates\": {\n",
        "            \"start\": {\n",
        "                \"lon\": -73.787442,\n",
        "                \"lat\": 40.641525\n",
        "            },\n",
        "            \"end\": {\n",
        "                \"lon\": -73.980072,\n",
        "                \"lat\": 40.742963\n",
        "            }\n",
        "        },\n",
        "        \"Rate_Code\": None,\n",
        "        \"store_and_forward\": None,\n",
        "        \"Payment\": {\n",
        "            \"type\": \"Credit\",\n",
        "            \"amt\": 20.5,\n",
        "            \"surcharge\": 0,\n",
        "            \"mta_tax\": None,\n",
        "            \"tip\": 9,\n",
        "            \"tolls\": 4.15,\n",
        "\t\t\t\t\t\t\"status\": \"booked\"\n",
        "        },\n",
        "        \"Passenger_Count\": 2,\n",
        "        \"passengers\": [\n",
        "            {\"name\": \"John\", \"rating\": 4.4},\n",
        "            {\"name\": \"Jack\", \"rating\": 3.6}\n",
        "        ],\n",
        "        \"Stops\": [\n",
        "            {\"lon\": -73.6, \"lat\": 40.6},\n",
        "            {\"lon\": -73.5, \"lat\": 40.5}\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "\n",
        "# define the connection to load to.\n",
        "# We now use duckdb, but you can switch to Bigquery later\n",
        "pipeline = dlt.pipeline(destination='duckdb', dataset_name='taxi_rides')\n",
        "\n",
        "# run the pipeline with default settings, and capture the outcome\n",
        "info = pipeline.run(data,\n",
        "\t\t\t\t\t\t\t\t\t\ttable_name=\"rides\",\n",
        "\t\t\t\t\t\t\t\t\t\twrite_disposition=\"replace\",\n",
        "                    primary_key='record_hash')\n",
        "\n",
        "# show the outcome\n",
        "\n",
        "conn = duckdb.connect(f\"{pipeline.pipeline_name}.duckdb\")\n",
        "\n",
        "# let's see the tables\n",
        "conn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\n",
        "print('Loaded tables: ')\n",
        "display(conn.sql(\"show tables\"))\n",
        "\n",
        "\n",
        "print(\"\\n\\n\\n Rides table below: Note the times are properly typed\")\n",
        "rides = conn.sql(\"SELECT * FROM rides\").df()\n",
        "display(rides)\n",
        "\n",
        "print(\"\\n\\n\\n Pasengers table\")\n",
        "passengers = conn.sql(\"SELECT * FROM rides__passengers\").df()\n",
        "display(passengers)\n",
        "print(\"\\n\\n\\n Stops table\")\n",
        "stops = conn.sql(\"SELECT * FROM rides__stops\").df()\n",
        "display(stops)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XXw1A9QUDUJ"
      },
      "source": [
        "# Bonus snippets\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp1tfRt1S4Gz"
      },
      "source": [
        "## Load to parquet file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sj2aTU-GUe2J"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install dlt[parquet] # Install dlt with all the necessary DuckDB dependencies\n",
        "%pip install parquet\n",
        "%mkdir .dlt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "YZ74NAUoV06Y",
        "outputId": "6510d2f6-04e6-4824-cc7f-640ef3ce6612"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import dlt\n",
        "import parquet\n",
        "import json\n",
        "import glob\n",
        "\n",
        "# Set the bucket_url. We can also use a local folder\n",
        "os.environ['DESTINATION__FILESYSTEM__BUCKET_URL'] = 'file:///content/.dlt/my_folder'\n",
        "\n",
        "url = \"https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl\"\n",
        "# Define your pipeline\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name='my_pipeline',\n",
        "    destination='filesystem',\n",
        "    dataset_name='mydata'\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Run the pipeline with the generator we created earlier.\n",
        "load_info = pipeline.run(stream_download_jsonl(url), table_name=\"users\", loader_file_format=\"parquet\")\n",
        "\n",
        "print(load_info)\n",
        "\n",
        "# Get a list of all Parquet files in the specified folder\n",
        "parquet_files = glob.glob('/content/.dlt/my_folder/mydata/users/*.parquet')\n",
        "\n",
        "# show parquet files\n",
        "for file in parquet_files:\n",
        "  print(file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoEk7pYXTBJe"
      },
      "source": [
        "## Load to bigquery\n",
        "To load to bigquery, we need credentials to bigquery.\n",
        "- dlt looks for credentials in several places as described in the [credential docs.](https://dlthub.com/docs/general-usage/credentials/configuration)\n",
        "- In the case of Bigquery you can read the docs [here](https://dlthub.com/docs/dlt-ecosystem/destinations/bigquery) for how to do it.\n",
        "- If you are running from Colab or a GCP machine, or you are authenticated with the gcp CLI, you can use these already-available local credentials. We will use the Colab Oauth here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIKdZERfxOYA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install dlt[bigquery]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQ3uWgqZxKS7"
      },
      "outputs": [],
      "source": [
        "# Authenticate to Google BigQuery\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bjVf9QXTA8R",
        "outputId": "f3640371-adc9-4832-854a-8b906c399b59"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import dlt\n",
        "\n",
        "os.environ['GOOGLE_CLOUD_PROJECT'] = 'dlt-dev-external'\n",
        "\n",
        "\n",
        "# Define your pipeline\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name='my_pipeline',\n",
        "    destination='bigquery',\n",
        "    dataset_name='dtc'\n",
        ")\n",
        "\n",
        "# Run the pipeline\n",
        "load_info = pipeline.run(stream_download_jsonl(url), table_name=\"users\")\n",
        "\n",
        "print(load_info)\n",
        "\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Construct a BigQuery client object.\n",
        "client = bigquery.Client()\n",
        "\n",
        "query = \"\"\"\n",
        "    SELECT *\n",
        "    FROM `dtc.users`\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)  # Make an API request.\n",
        "\n",
        "print(\"The query data:\")\n",
        "for row in query_job:\n",
        "    # Row values can be accessed by field name or index.\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bftPmKJYS_7J"
      },
      "source": [
        "## Other demos\n",
        "Find more demos in this repo, or look on our blog for multiple community demos\n",
        "* https://github.com/dlt-hub/dlt_demos\n",
        "* https://dlthub.com/docs/blog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aPjk0O3S_Ag"
      },
      "source": [
        "## Docs Links\n",
        "\n",
        "You will find more info about advanced capabilities of dlt [in this build a pipeline guide](**https**://dlthub.com/docs/build-a-pipeline-tutorial)\n",
        "\n",
        "If you would like to join our community, find the slack join link at the top of the docs.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3RGdjQnN7J8A"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
