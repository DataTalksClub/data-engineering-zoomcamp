{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import io\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Path to your JSON key file\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"gcs.json\"\n",
    "\n",
    "# Base URL for the Parquet files\n",
    "BASE_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-\"\n",
    "MONTHS = [f\"{i:02d}\" for i in range(1, 7)]  # ['01', '02', ..., '06']\n",
    "\n",
    "# Define a dlt resource for fetching Parquet data\n",
    "@dlt.resource(name=\"ny_taxi_dlt\", write_disposition=\"replace\")\n",
    "def paginated_getter():\n",
    "    \"\"\"Fetches and yields monthly Parquet data as Pandas DataFrames.\"\"\"\n",
    "\n",
    "    for month in MONTHS:\n",
    "        url = f\"{BASE_URL}{month}.parquet\"\n",
    "        \n",
    "        try:\n",
    "            # Fetch the Parquet file in streaming mode\n",
    "            with requests.get(url, stream=True) as response:\n",
    "                response.raise_for_status()  # Raise an error for failed requests\n",
    "\n",
    "                # Read file in chunks and store in a buffer\n",
    "                buffer = io.BytesIO()\n",
    "                for chunk in response.iter_content(chunk_size=1024 * 1024):  # Read in 1MB chunks\n",
    "                    buffer.write(chunk)\n",
    "\n",
    "                buffer.seek(0)  # Reset buffer position\n",
    "\n",
    "                # Read Parquet file using pyarrow and convert to Pandas DataFrame\n",
    "                table = pq.read_table(buffer)\n",
    "\n",
    "                print(f'Got month {month} with {len(table)} records')\n",
    "\n",
    "                if table.num_rows > 0:  # If data exists, yield it\n",
    "                    yield table\n",
    "                else:\n",
    "                    break  # Stop if no more data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch data for month {month}: {e}\")\n",
    "\n",
    "# Create and configure the dlt pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"ny_taxi_pipeline_dlt\",\n",
    "    destination=\"bigquery\",\n",
    "    dataset_name=\"ny_taxi_parquet_dlt_8\",\n",
    "    dev_mode=True\n",
    ")\n",
    "\n",
    "# Run the pipeline and load data into BigQuery\n",
    "load_info = pipeline.run(paginated_getter())\n",
    "\n",
    "# Print load info and normalization details\n",
    "print(load_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-zoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
