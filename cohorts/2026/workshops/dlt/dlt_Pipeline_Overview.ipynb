{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Data Pipeline with dlt\n",
        "\n",
        "In this notebook, we will build a complete data pipeline from scratch using **dlt**.\n",
        "\n",
        "Our goal is simple:\n",
        "\n",
        "‚Üí Fetch real data from an API  \n",
        "‚Üí Turn it into clean relational tables  \n",
        "‚Üí Load it into a database  \n",
        "‚Üí Explore and analyze it  \n",
        "\n",
        "We will use the **Open Library API** as our data source and **DuckDB** as our database.\n",
        "\n",
        "Along the way, you will learn:\n",
        "\n",
        "- What a dlt source is  \n",
        "- What a dlt pipeline does  \n",
        "- How data moves through Extract ‚Üí Normalize ‚Üí Load  \n",
        "- How to inspect and explore the final dataset  \n",
        "\n",
        "By the end, you will understand not just how to run a pipeline, but what happens at each stage.\n"
      ],
      "metadata": {
        "id": "bPVVve29bu6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Step 0: Install dependencies\n"
      ],
      "metadata": {
        "id": "u9eCv60qV5PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install dependencies first\n",
        "!pip -q install dlt[duckdb]"
      ],
      "metadata": {
        "id": "Arp4d7KZNRTS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>In this notebook we will use:</p>\n",
        "\n",
        "<ul>\n",
        "  <li><strong>dlt</strong> to extract, normalize, and load data</li>\n",
        "  <li><strong>DuckDB</strong> as the destination database (runs locally inside Colab)</li>\n",
        "</ul>\n",
        "\n",
        "<p>\n",
        "  DuckDB is great for beginners because it requires no setup and no credentials.\n",
        "</p>"
      ],
      "metadata": {
        "id": "x7VGYS5hWNKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### 1Ô∏è‚É£  Step 1: Import libraries"
      ],
      "metadata": {
        "id": "aQTSvnvnHWBd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<p>In this cell we import the libraries we will use throughout the notebook:</p>\n",
        "\n",
        "<ul>\n",
        "  <li><strong>dlt</strong> is the main library for building and running the pipeline</li>\n",
        "  <li><strong>rest_api_source</strong> helps us define an API source using a simple configuration</li>\n",
        "  <li><strong>islice</strong> (from <code>itertools</code>) is a small Python helper for previewing only a few records</li>\n",
        "</ul>\n"
      ],
      "metadata": {
        "id": "YFQGLTECWkpn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Lm8AbbHBImjI"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "import dlt\n",
        "from itertools import islice\n",
        "from dlt.sources.rest_api import rest_api_source"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2Ô∏è‚É£ Step 2: Define the API source (Open Library)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UFoBTwDVhzRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>\n",
        "  In <strong>dlt</strong>, a <strong>source</strong> is the part of your pipeline that knows how to fetch data from somewhere.\n",
        "  In this notebook, our source fetches data from the <strong>Open Library Search API</strong>.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "  We define the source using <code>rest_api_source</code>, which lets us describe an API in a simple\n",
        "  Python dictionary instead of writing lots of request code.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "  üìñ <strong>Open Library Search API docs:</strong><br>\n",
        "  <a href=\"https://openlibrary.org/dev/docs/api/search\" target=\"_blank\">\n",
        "    https://openlibrary.org/dev/docs/api/search\n",
        "  </a>\n",
        "</p>"
      ],
      "metadata": {
        "id": "VdKrEM-VXEY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def openlibrary_source(query: str = \"harry potter\"):\n",
        "\n",
        "    return rest_api_source({\n",
        "        \"client\": {\n",
        "            \"base_url\": \"https://openlibrary.org\",\n",
        "        },\n",
        "        \"resource_defaults\": {\n",
        "            \"primary_key\": \"key\",\n",
        "            \"write_disposition\": \"replace\",\n",
        "        },\n",
        "        \"resources\": [\n",
        "            {\n",
        "                \"name\": \"books\",\n",
        "                \"endpoint\": {\n",
        "                    \"path\": \"search.json\",\n",
        "                    \"params\": {\n",
        "                        \"q\": query,\n",
        "                        \"limit\": 100,\n",
        "                    },\n",
        "                    \"data_selector\": \"docs\",\n",
        "                    \"paginator\": {\n",
        "                        \"type\": \"offset\",\n",
        "                        \"limit\": 100,\n",
        "                        \"offset_param\": \"offset\",\n",
        "                        \"limit_param\": \"limit\",\n",
        "                        \"total_path\": \"numFound\",\n",
        "                    },\n",
        "                },\n",
        "            },\n",
        "        ],\n",
        "    })\n"
      ],
      "metadata": {
        "id": "hOxkEKy4Kaj4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3Ô∏è‚É£ Step 3: Create the dlt pipeline"
      ],
      "metadata": {
        "id": "ntKAVaEGYFgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"ol_demo\",\n",
        "    destination=\"duckdb\",\n",
        "    dataset_name=\"ol_data\",\n",
        ")"
      ],
      "metadata": {
        "id": "bxpFEetGh3lS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Step 3: Pipeline Breakdown\n",
        "\n",
        "At this point we have defined two key building blocks:\n",
        "\n",
        "- **The source** describes where the data comes from and how to fetch it from the API.  \n",
        "- **The pipeline** describes where the data should go (DuckDB) and keeps track of tables, schemas, and run history.  \n",
        "\n",
        "---\n",
        "\n",
        "Instead of running everything at once, we will now run the pipeline in three separate phases so you can clearly see what happens at each stage:\n",
        "\n",
        "1. **Extract**: download raw data from the API  \n",
        "2. **Normalize**: turn nested JSON into relational tables  \n",
        "3. **Load**: write those tables into DuckDB  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y7CJ9A2HXsFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once these steps make sense, we will run the full workflow again using one command:\n",
        "\n",
        "```python\n",
        "pipeline.run()\n"
      ],
      "metadata": {
        "id": "pAYgUUJIw-c4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Step 4: Extract data from the API\n",
        "\n",
        "Now we run the first stage of the pipeline: **Extract**.\n",
        "\n",
        "Extract means:\n",
        "\n",
        "- dlt sends requests to the Open Library API\n",
        "- the raw JSON responses are downloaded\n",
        "- the results are stored in dlt‚Äôs local working folder\n",
        "\n",
        "At this stage, the data is **not** in DuckDB yet. We are just confirming that we successfully pulled data from the API."
      ],
      "metadata": {
        "id": "JsfcBA7McJMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extract_info = pipeline.extract(openlibrary_source())"
      ],
      "metadata": {
        "id": "yifCIPxSKJZ4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### What we will print\n",
        "\n",
        "After extraction, we will print a small summary showing:\n",
        "\n",
        "- which **resources** were extracted\n",
        "- which **tables** will be created later\n",
        "- how many rows were extracted per resource\n",
        "\n",
        "This helps confirm that the pipeline is working before we move on to normalization."
      ],
      "metadata": {
        "id": "NLRRVLnLcNgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "load_id = extract_info.loads_ids[-1]\n",
        "m = extract_info.metrics[load_id][0]\n",
        "\n",
        "print(\"Resources:\", list(m[\"resource_metrics\"].keys()))\n",
        "print(\"Tables:\", list(m[\"table_metrics\"].keys()))\n",
        "extract_info = pipeline.extract(openlibrary_source())\n",
        "print(\"Load ID:\", load_id)\n",
        "print()\n",
        "\n",
        "for resource, rm in m[\"resource_metrics\"].items():\n",
        "    print(f\"Resource: {resource}\")\n",
        "    print(f\"rows extracted: {rm.items_count}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtDasHRNNNN0",
        "outputId": "51c71eeb-5435-40a1-8728-ea48c59bfd58"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resources: ['books']\n",
            "Tables: ['books']\n",
            "Load ID: 1770819724.7555451\n",
            "\n",
            "Resource: books\n",
            "  rows extracted: 3756\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What you should see after Extract\n",
        "\n",
        "In our case, Extract shows only **one resource and one table**:\n",
        "\n",
        "- **Resources:** `['books']`  \n",
        "- **Tables:** `['books']`\n",
        "\n",
        "That is expected.\n",
        "\n",
        "The `search` endpoint returns a list of book results, so dlt stores those rows in a single table called `search`. The interesting part comes next, because many fields inside each row are lists or nested objects. Those will turn into additional tables during **Normalize**.\n",
        "\n",
        "Example output:\n",
        "\n",
        "- **25 rows extracted** means we pulled 25 search results (books)  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "f6MwYtznc3UX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Step 5: Normalize\n",
        "\n",
        "Now we run **Normalize**.\n",
        "\n",
        "APIs often return nested JSON. For example, a book can have:\n",
        "\n",
        "- multiple authors (a list)\n",
        "- multiple editions (a nested object with its own list)\n",
        "- multiple identifiers (another list)\n",
        "\n",
        "Normalize converts that nested structure into separate tables so the data becomes easier to query.\n",
        "\n",
        "You will usually see new child tables appear with names like:\n",
        "\n",
        "- `books__author_name`\n",
        "- `books__editions__docs`\n",
        "- `books__editions__docs__language`\n",
        "\n",
        "These tables are linked back to the main `books` table using internal dlt ID columns.\n",
        "\n",
        "In the next cell, we will print a short summary showing which tables were created and how many rows were written to each table.\n"
      ],
      "metadata": {
        "id": "lQVLZMcyXWkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalize_info = pipeline.normalize()"
      ],
      "metadata": {
        "id": "LCmiiG3tXXwh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_id = normalize_info.loads_ids[-1]\n",
        "m = normalize_info.metrics[load_id][0]\n",
        "\n",
        "print(\"Load ID:\", load_id)\n",
        "print()\n",
        "\n",
        "print(\"Tables created/updated:\")\n",
        "for table_name, tm in m[\"table_metrics\"].items():\n",
        "    # skip dlt internal tables to keep it beginner-friendly\n",
        "    if table_name.startswith(\"_dlt\"):\n",
        "        continue\n",
        "    print(f\"  - {table_name}: {tm.items_count} rows\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kNiY112Xvuk",
        "outputId": "502bff6b-edb2-4bd8-a9e9-1f1b88f20c48"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load ID: 1770819807.3935525\n",
            "\n",
            "Tables created/updated:\n",
            "  - books: 3756 rows\n",
            "  - books__author_key: 4600 rows\n",
            "  - books__author_name: 4600 rows\n",
            "  - books__ia: 3422 rows\n",
            "  - books__ia_collection: 2724 rows\n",
            "  - books__language: 3748 rows\n",
            "  - books__id_standard_ebooks: 12 rows\n",
            "  - books__id_librivox: 60 rows\n",
            "  - books__id_project_gutenberg: 54 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What happened during Normalize?\n",
        "\n",
        "After running `pipeline.normalize()`, we now see multiple tables instead of just one.\n",
        "\n",
        "Tables created/updated:\n",
        "\n",
        "- `books`: 25 rows  \n",
        "- `books__author_key`: 29 rows  \n",
        "- `books__author_name`: 29 rows  \n",
        "- `books__editions__docs`: 25 rows  \n",
        "- `books__editions__docs__language`: 24 rows  \n",
        "- `books__ia`: 82 rows  \n",
        "\n",
        "---\n",
        "\n",
        "### What does this mean?\n",
        "\n",
        "We started with **25 book search results** in the `search` table.\n",
        "\n",
        "During normalization:\n",
        "\n",
        "- Each book may have **multiple authors**, so those were split into:\n",
        "  - `books__author_name`\n",
        "  - `books__author_key`\n",
        "\n",
        "- Each book may contain **edition information**, which became:\n",
        "  - `books__editions__docs`\n",
        "\n",
        "- Some editions contain **language information**, which became:\n",
        "  - `books__editions__docs__language`\n",
        "\n",
        "- The `ia` field (Internet Archive IDs) is a list, so it became:\n",
        "  - `books__ia`\n",
        "\n",
        "This is the key moment in the pipeline.\n",
        "\n",
        "The data has been transformed from nested JSON into a **relational structure** with multiple linked tables. This makes it much easier to query and analyze.\n"
      ],
      "metadata": {
        "id": "ctHuJ0yEdNaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6Ô∏è‚É£ Step 6: Load\n",
        "\n",
        "Now we run the final stage of the pipeline: **Load**.\n",
        "\n",
        "Load means:\n",
        "\n",
        "- dlt creates tables in DuckDB (if they do not already exist)\n",
        "- the normalized rows are inserted into those tables\n",
        "- the pipeline records the load in its internal tracking tables\n",
        "\n"
      ],
      "metadata": {
        "id": "lJ5QzSnYdidK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_info = pipeline.load()"
      ],
      "metadata": {
        "id": "d9Xb67c5XfL5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "After this step, the data is fully stored in the database and ready to query.\n",
        "\n",
        "At this point:\n",
        "\n",
        "- The `books` table contains our books\n",
        "- The related tables (such as `books__author_name` and `books__editions__docs`) contain the exploded nested data\n",
        "- Everything is now queryable using `pipeline.dataset()` or SQL\n",
        "\n",
        "This is the moment where the data officially moves from ‚Äúpipeline processing‚Äù into a database you can explore."
      ],
      "metadata": {
        "id": "ehkz8lESGGdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7Ô∏è‚É£ Run the entire pipeline in one command\n",
        "\n",
        "Now that we have walked through each step individually, we can run the entire workflow using a single command:\n",
        "\n"
      ],
      "metadata": {
        "id": "jBznxM00eCOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_info = pipeline.run(openlibrary_source())"
      ],
      "metadata": {
        "id": "YQLigkh-f7Ey"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>What does <code>pipeline.run()</code> do?</h3>\n",
        "\n",
        "<p>\n",
        "  <code>pipeline.run()</code> simply combines the three steps we already executed manually:\n",
        "</p>\n",
        "\n",
        "<ol>\n",
        "  <li><strong>Extract</strong> ‚Äì fetch data from the Open Library API</li>\n",
        "  <li><strong>Normalize</strong> ‚Äì convert nested JSON into relational tables</li>\n",
        "  <li><strong>Load</strong> ‚Äì write those tables into DuckDB</li>\n",
        "</ol>\n",
        "\n",
        "<p>In other words, this:</p>\n",
        "\n",
        "<pre><code>pipeline.run(source)</code></pre>\n",
        "\n",
        "<p>is equivalent to:</p>\n",
        "\n",
        "<pre><code>pipeline.extract(source)\n",
        "pipeline.normalize()\n",
        "pipeline.load()</code></pre>\n",
        "\n",
        "<p>\n",
        "  There is no hidden magic. It just runs the full ELT process in order.\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "SbLkA8W7eNPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8Ô∏è‚É£ Inspect the loaded data (the fun part)\n",
        "\n",
        "Now that the data is loaded into DuckDB, we can inspect it using `pipeline.dataset()`.\n",
        "\n",
        "This gives us a convenient Python interface for exploring the tables that dlt created, without writing SQL.\n",
        "\n",
        "---\n",
        "\n",
        "### List available tables\n",
        "\n",
        "First, let‚Äôs see what tables exist in the dataset:"
      ],
      "metadata": {
        "id": "7ViMq6gIfJj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = pipeline.dataset()"
      ],
      "metadata": {
        "id": "bmnrK1aVZXPO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds.tables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SV6J6AtBf0xq",
        "outputId": "19ad26bf-f34a-4f8e-c30c-5acd3342c3c5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['books',\n",
              " 'books__author_key',\n",
              " 'books__author_name',\n",
              " 'books__ia',\n",
              " 'books__ia_collection',\n",
              " 'books__language',\n",
              " 'books__id_standard_ebooks',\n",
              " 'books__id_librivox',\n",
              " 'books__id_project_gutenberg',\n",
              " '_dlt_version',\n",
              " '_dlt_loads',\n",
              " '_dlt_pipeline_state']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = ds.books.df()      # main table\n",
        "df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "WLa4yN7lf1TF",
        "outputId": "d2da841b-a8bf-461f-a011-eb1db644656f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  cover_edition_key   cover_i   ebook_access  edition_count  \\\n",
              "0       OL61027601M  15155833     borrowable            396   \n",
              "1       OL26378158M  15158660  printdisabled            144   \n",
              "2       OL26234270M  10580435     borrowable            278   \n",
              "\n",
              "   first_publish_year  has_fulltext              key lending_edition_s  \\\n",
              "0                1997          True  /works/OL82563W       OL38565767M   \n",
              "1                2007          True  /works/OL82586W              None   \n",
              "2                1999          True  /works/OL82536W       OL48101764M   \n",
              "\n",
              "            lending_identifier_s  public_scan_b  \\\n",
              "0  harrypotterylapi0000rowl_q5r6          False   \n",
              "1                           None          False   \n",
              "2                 bdrc-W8LS66814          False   \n",
              "\n",
              "                                      title        _dlt_load_id  \\\n",
              "0  Harry Potter and the Philosopher's Stone  1770819876.9353185   \n",
              "1      Harry Potter and the Deathly Hallows  1770819876.9353185   \n",
              "2  Harry Potter and the Prisoner of Azkaban  1770819876.9353185   \n",
              "\n",
              "          _dlt_id subtitle  \n",
              "0  lGJrV2BS8Z9qJQ     None  \n",
              "1  F9W0WQlLwgvsFw     None  \n",
              "2  kSdfO1XbBVAjmQ     None  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-78b9c6b5-669d-4905-9f29-f9885ad30a9d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cover_edition_key</th>\n",
              "      <th>cover_i</th>\n",
              "      <th>ebook_access</th>\n",
              "      <th>edition_count</th>\n",
              "      <th>first_publish_year</th>\n",
              "      <th>has_fulltext</th>\n",
              "      <th>key</th>\n",
              "      <th>lending_edition_s</th>\n",
              "      <th>lending_identifier_s</th>\n",
              "      <th>public_scan_b</th>\n",
              "      <th>title</th>\n",
              "      <th>_dlt_load_id</th>\n",
              "      <th>_dlt_id</th>\n",
              "      <th>subtitle</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OL61027601M</td>\n",
              "      <td>15155833</td>\n",
              "      <td>borrowable</td>\n",
              "      <td>396</td>\n",
              "      <td>1997</td>\n",
              "      <td>True</td>\n",
              "      <td>/works/OL82563W</td>\n",
              "      <td>OL38565767M</td>\n",
              "      <td>harrypotterylapi0000rowl_q5r6</td>\n",
              "      <td>False</td>\n",
              "      <td>Harry Potter and the Philosopher's Stone</td>\n",
              "      <td>1770819876.9353185</td>\n",
              "      <td>lGJrV2BS8Z9qJQ</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OL26378158M</td>\n",
              "      <td>15158660</td>\n",
              "      <td>printdisabled</td>\n",
              "      <td>144</td>\n",
              "      <td>2007</td>\n",
              "      <td>True</td>\n",
              "      <td>/works/OL82586W</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "      <td>Harry Potter and the Deathly Hallows</td>\n",
              "      <td>1770819876.9353185</td>\n",
              "      <td>F9W0WQlLwgvsFw</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>OL26234270M</td>\n",
              "      <td>10580435</td>\n",
              "      <td>borrowable</td>\n",
              "      <td>278</td>\n",
              "      <td>1999</td>\n",
              "      <td>True</td>\n",
              "      <td>/works/OL82536W</td>\n",
              "      <td>OL48101764M</td>\n",
              "      <td>bdrc-W8LS66814</td>\n",
              "      <td>False</td>\n",
              "      <td>Harry Potter and the Prisoner of Azkaban</td>\n",
              "      <td>1770819876.9353185</td>\n",
              "      <td>kSdfO1XbBVAjmQ</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-78b9c6b5-669d-4905-9f29-f9885ad30a9d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-78b9c6b5-669d-4905-9f29-f9885ad30a9d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-78b9c6b5-669d-4905-9f29-f9885ad30a9d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3756,\n  \"fields\": [\n    {\n      \"column\": \"cover_edition_key\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1192,\n        \"samples\": [\n          \"OL24951484M\",\n          \"OL9131663M\",\n          \"OL47198575M\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cover_i\",\n      \"properties\": {\n        \"dtype\": \"Int64\",\n        \"num_unique_values\": 1288,\n        \"samples\": [\n          842156,\n          10365881,\n          3341732\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ebook_access\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"printdisabled\",\n          \"unclassified\",\n          \"no_ebook\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"edition_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 108,\n        \"min\": 0,\n        \"max\": 3546,\n        \"num_unique_values\": 62,\n        \"samples\": [\n          44,\n          92,\n          396\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"first_publish_year\",\n      \"properties\": {\n        \"dtype\": \"Int64\",\n        \"num_unique_values\": 127,\n        \"samples\": [\n          2008,\n          1622,\n          1962\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"has_fulltext\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"key\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3756,\n        \"samples\": [\n          \"/works/OL34662215W\",\n          \"/works/OL39702699W\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lending_edition_s\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 281,\n        \"samples\": [\n          \"OL45637056M\",\n          \"OL26064272M\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lending_identifier_s\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 281,\n        \"samples\": [\n          \"alicesadventures0000unse_v7d2\",\n          \"harrypottermagic0000unse_n5w6\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"public_scan_b\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2984,\n        \"samples\": [\n          \"1000 Facts and Trivia about Marvel Cinematic Universe, Game of Thrones, Disney, Star Wars, Harry Potter 1\",\n          \"The Unofficial Harry Potter Insults Handbook\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"_dlt_load_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"1770819876.9353185\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"_dlt_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3756,\n        \"samples\": [\n          \"ZN3UfCkWBXFxSw\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subtitle\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 59,\n        \"samples\": [\n          \"Hogwarts Through the Years\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9Ô∏è‚É£ dlt makes this easier‚Ä¶ but not effortless\n",
        "\n",
        "What dlt already handled for us:\n",
        "\n",
        "‚úî API requests  \n",
        "‚úî JSON normalization  \n",
        "‚úî Table creation  \n",
        "‚úî Database loading  \n",
        "‚úî Simple dataset inspection  \n",
        "\n",
        "---\n",
        "\n",
        "But there are still friction points:\n",
        "\n",
        "‚Ä¢ Getting the REST API config exactly right  \n",
        "‚Ä¢ Remembering paginator syntax  \n",
        "‚Ä¢ Remembering how to inspect tables  \n",
        "‚Ä¢ Writing Python or SQL to get insights  \n",
        "‚Ä¢ Debugging schema or pagination issues  \n",
        "\n",
        "It works‚Ä¶ but it still takes effort.\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ The next step: LLM-powered workflows\n",
        "\n",
        "dlt now integrates LLMs directly into the workflow to make:\n",
        "\n",
        "‚Ä¢ Pipeline runs easier  \n",
        "‚Ä¢ Debugging faster  \n",
        "‚Ä¢ Schema inspection simpler  \n",
        "‚Ä¢ Data analysis more natural  \n",
        "\n",
        "Instead of writing glue code, you can use natural language.\n",
        "\n",
        "In the next demo, we will see what that looks like.\n"
      ],
      "metadata": {
        "id": "OWFqaH2wgCWR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BweSVO3igErN"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}