# From APIs to Warehouses: AI-Assisted Data Ingestion with dlt

[Register for the workshop](https://luma.com/hzis1yzp)

This hands-on workshop focuses on building reliable data ingestion pipelines to data warehouses (for example, Snowflake) using dlt (data load tool), enhanced with LLMs, the dlt dashboard, and dlt MCP.

## What you'll learn

You'll work through the key building blocks of a production-ready ingestion setup, including:

- Extracting data from APIs, files, and databases
- Normalizing data into consistent schemas
- Writing data to a data warehouse (e.g. Snowflake)
- Using LLMs to accelerate dlt pipeline development
- Validating data and schema changes using the dlt dashboard and dlt MCP

The session is fully practical and code-driven. By the end of the workshop, you'll understand how to design maintainable, scalable ingestion pipelines and use AI and validation tools to build them faster and with confidence.

## Materials

* [Workshop instructions](dlt/README.md)
* [dlt Pipeline Overview Notebook (Google Colab)](https://colab.research.google.com/github/anair123/data-engineering-zoomcamp/blob/workshop/dlt_2026/cohorts/2026/workshops/dlt/dlt_Pipeline_Overview.ipynb)
* [Homework](dlt/dlt_homework.md)
* [Homework submission form](https://courses.datatalks.club/de-zoomcamp-2026/homework/dlt)

## About the Speaker

**Aashish Nair** is a Data Engineer at dltHub and the creator of the famous _dlt deployment_ course, where he teaches best practices for running dlt pipelines in production.
